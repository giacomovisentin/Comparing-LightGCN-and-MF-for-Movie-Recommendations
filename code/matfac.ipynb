{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gf53GzgVALb0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class ImprovedMatrixFactorizationWithBias:\n",
        "    def __init__(self,\n",
        "                 alpha=0.001,\n",
        "                 iterations=20,\n",
        "                 num_latent=100,\n",
        "                 lambda_reg=0.02,\n",
        "                 num_users=943,\n",
        "                 num_items=1682,\n",
        "                 early_stopping=5):\n",
        "        \"\"\"\n",
        "        Initialize the model with hyperparameters and configuration.\n",
        "        \"\"\"\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.iterations = iterations  # Number of iterations for training\n",
        "        self.num_latent = num_latent  # Number of latent factors\n",
        "        self.lambda_reg = lambda_reg  # Regularization parameter\n",
        "        self.num_users = num_users  # Total number of users\n",
        "        self.num_items = num_items  # Total number of items\n",
        "        self.early_stopping = early_stopping  # Patience for early stopping\n",
        "\n",
        "    def fit(self, train_data, validation_data=None):\n",
        "        \"\"\"\n",
        "        Train the model using training data with optional validation data.\n",
        "        \"\"\"\n",
        "        self.train = train_data\n",
        "\n",
        "        # Initialize latent factors P and Q using Xavier/Glorot initialization\n",
        "        limit = np.sqrt(6 / (self.num_users + self.num_items))\n",
        "        self.P = np.random.uniform(-limit, limit, (self.num_users, self.num_latent))\n",
        "        self.Q = np.random.uniform(-limit, limit, (self.num_items, self.num_latent))\n",
        "\n",
        "        # Initialize biases with small random values\n",
        "        self.bu = np.random.normal(0, 0.01, self.num_users)\n",
        "        self.bi = np.random.normal(0, 0.01, self.num_items)\n",
        "        self.global_bias = np.mean([r for _, _, r in train_data])  # Mean rating as global bias\n",
        "\n",
        "        best_rmse = float('inf')  # Best RMSE on validation set\n",
        "        patience = self.early_stopping  # Remaining patience for early stopping\n",
        "        rmse_history = []  # To store RMSE values for each iteration\n",
        "\n",
        "        for iteration in range(self.iterations):\n",
        "            np.random.shuffle(self.train)  # Shuffle training data\n",
        "            squared_error = 0  # Accumulate squared errors\n",
        "            count = 0  # Counter for processed ratings\n",
        "\n",
        "            # Mini-batch gradient descent\n",
        "            batch_size = 1000\n",
        "            for i in range(0, len(self.train), batch_size):\n",
        "                batch = self.train[i:i + batch_size]\n",
        "\n",
        "                for user_id, item_id, rating in batch:\n",
        "                    # Predict the rating for the given user-item pair\n",
        "                    pred = self._predict_single(user_id, item_id)\n",
        "                    error = rating - pred\n",
        "\n",
        "                    squared_error += error ** 2\n",
        "                    count += 1\n",
        "\n",
        "                    # Gradient descent with momentum initialization\n",
        "                    if not hasattr(self, 'P_momentum'):\n",
        "                        self.P_momentum = np.zeros_like(self.P)\n",
        "                        self.Q_momentum = np.zeros_like(self.Q)\n",
        "                        self.bu_momentum = np.zeros_like(self.bu)\n",
        "                        self.bi_momentum = np.zeros_like(self.bi)\n",
        "\n",
        "                    momentum = 0.9  # Momentum factor\n",
        "\n",
        "                    # Update latent factors P\n",
        "                    grad_P = 2 * error * self.Q[item_id] - self.lambda_reg * self.P[user_id]\n",
        "                    self.P_momentum[user_id] = momentum * self.P_momentum[user_id] + self.alpha * grad_P\n",
        "                    self.P[user_id] += self.P_momentum[user_id]\n",
        "\n",
        "                    # Update latent factors Q\n",
        "                    grad_Q = 2 * error * self.P[user_id] - self.lambda_reg * self.Q[item_id]\n",
        "                    self.Q_momentum[item_id] = momentum * self.Q_momentum[item_id] + self.alpha * grad_Q\n",
        "                    self.Q[item_id] += self.Q_momentum[item_id]\n",
        "\n",
        "                    # Update user bias bu\n",
        "                    grad_bu = 2 * error - self.lambda_reg * self.bu[user_id]\n",
        "                    self.bu_momentum[user_id] = momentum * self.bu_momentum[user_id] + self.alpha * grad_bu\n",
        "                    self.bu[user_id] += self.bu_momentum[user_id]\n",
        "\n",
        "                    # Update item bias bi\n",
        "                    grad_bi = 2 * error - self.lambda_reg * self.bi[item_id]\n",
        "                    self.bi_momentum[item_id] = momentum * self.bi_momentum[item_id] + self.alpha * grad_bi\n",
        "                    self.bi[item_id] += self.bi_momentum[item_id]\n",
        "\n",
        "            # Compute RMSE for training data\n",
        "            rmse = np.sqrt(squared_error / count)\n",
        "            rmse_history.append(rmse)\n",
        "\n",
        "            # Validation check\n",
        "            if validation_data is not None:\n",
        "                val_rmse = np.sqrt(np.mean([(r - self._predict_single(u, i)) ** 2\n",
        "                                            for u, i, r in validation_data]))\n",
        "                print(f\"Iteration {iteration + 1}/{self.iterations}, Train RMSE: {rmse:.4f}, Val RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "                if val_rmse < best_rmse:\n",
        "                    # Save the best parameters\n",
        "                    best_rmse = val_rmse\n",
        "                    patience = self.early_stopping\n",
        "                    self.best_P = self.P.copy()\n",
        "                    self.best_Q = self.Q.copy()\n",
        "                    self.best_bu = self.bu.copy()\n",
        "                    self.best_bi = self.bi.copy()\n",
        "                else:\n",
        "                    patience -= 1\n",
        "                    if patience == 0:\n",
        "                        print(\"Early stopping!\")\n",
        "                        # Restore the best parameters\n",
        "                        self.P = self.best_P\n",
        "                        self.Q = self.best_Q\n",
        "                        self.bu = self.best_bu\n",
        "                        self.bi = self.best_bi\n",
        "                        break\n",
        "            else:\n",
        "                print(f\"Iteration {iteration + 1}/{self.iterations}, RMSE: {rmse:.4f}\")\n",
        "\n",
        "            # Decay learning rate\n",
        "            self.alpha *= 0.995\n",
        "\n",
        "        self.rmse_history = rmse_history  # Save RMSE history\n",
        "\n",
        "    def _predict_single(self, user_id, item_id):\n",
        "        \"\"\"\n",
        "        Predict the rating for a single user-item pair.\n",
        "        \"\"\"\n",
        "        pred = (self.Q[item_id] @ self.P[user_id] +\n",
        "                self.bu[user_id] +\n",
        "                self.bi[item_id] +\n",
        "                self.global_bias)\n",
        "        # Clip predictions to be within the allowed range\n",
        "        return np.clip(pred, 1, 5)\n",
        "\n",
        "    def predict(self, test_data):\n",
        "        \"\"\"\n",
        "        Predict ratings for a set of test user-item pairs.\n",
        "        \"\"\"\n",
        "        return [self._predict_single(u, i) for u, i, _ in test_data]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare data for training\n",
        "def prepare_data(df_data):\n",
        "    # Convert the data into a list of tuples (userId, movieId, rating)\n",
        "    data = [(row.userId, row.movieId, row.rating) for idx, row in df_data.iterrows()]\n",
        "    # Split the data into training and test sets\n",
        "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "    return train_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_for_user(clf, user_id, movie_ids):\n",
        "    \"\"\"\n",
        "    Generates a sorted list of predictions for a specific user.\n",
        "\n",
        "    Args:\n",
        "        clf: Instance of ImprovedMatrixFactorizationWithBias.\n",
        "        user_id: ID of the user for whom predictions are being made.\n",
        "        movie_ids: List of movie IDs to consider for predictions.\n",
        "\n",
        "    Returns:\n",
        "        List of tuples (movie_id, predicted_rating) sorted in descending order of ratings.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    # Loop through the provided movie IDs\n",
        "    for movie_id in movie_ids:\n",
        "        # Predict the rating for the user-movie pair using the model's single prediction method\n",
        "        pred = clf._predict_single(user_id, movie_id)\n",
        "        predictions.append((movie_id, pred))  # Store the movie ID and its predicted rating as a tuple\n",
        "\n",
        "    # Sort the predictions by rating in descending order\n",
        "    return sorted(predictions, key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_all_metrics(model, test_data, n_movies, k=10):\n",
        "    \"\"\"\n",
        "    Calculates Recall@K, Precision@K, NDCG@K, Gini Index, and Coverage for a recommendation system.\n",
        "\n",
        "    Args:\n",
        "        model: Recommendation model (e.g., ImprovedMatrixFactorizationWithBias).\n",
        "        test_data: Test dataset in the format (user_id, movie_id, rating).\n",
        "        n_movies: Total number of movies in the dataset.\n",
        "        k: Number of recommendations to consider.\n",
        "\n",
        "    Returns:\n",
        "        metrics: Dictionary containing all calculated metrics.\n",
        "    \"\"\"\n",
        "    # Group true items (movies with high ratings) for each user\n",
        "    user_true_items = {}\n",
        "    for user, movie, rating in test_data:\n",
        "        if user not in user_true_items:\n",
        "            user_true_items[user] = []\n",
        "        if rating >= 3.5:  # Only consider high ratings as \"true\" items\n",
        "            user_true_items[user].append(movie)\n",
        "\n",
        "    recommendations = {}\n",
        "    item_counts = np.zeros(n_movies)  # For calculating Coverage and Gini Index\n",
        "\n",
        "    recall_list = []\n",
        "    precision_list = []\n",
        "    ndcg_list = []\n",
        "\n",
        "    for user, true_items in user_true_items.items():\n",
        "        # Generate recommendations for the user\n",
        "        recommended_items = predict_for_user(model, user, range(n_movies))\n",
        "        top_k_items = [item[0] for item in recommended_items[:k]]  # Extract top-K item IDs\n",
        "        recommendations[user] = top_k_items\n",
        "\n",
        "        # Update the frequency of recommended items\n",
        "        for item in top_k_items:\n",
        "            item_counts[item] += 1\n",
        "\n",
        "        # Calculate user-specific metrics\n",
        "        tp = len(set(top_k_items) & set(true_items))  # True positives\n",
        "\n",
        "        # Recall@K: Proportion of true items retrieved in the top-K recommendations\n",
        "        recall = tp / len(true_items) if true_items else 0\n",
        "        recall_list.append(recall)\n",
        "\n",
        "        # Precision@K: Proportion of top-K recommendations that are true items\n",
        "        precision = tp / k\n",
        "        precision_list.append(precision)\n",
        "\n",
        "        # NDCG@K: Normalized Discounted Cumulative Gain\n",
        "        dcg = 0\n",
        "        idcg = 0\n",
        "        for i, item in enumerate(top_k_items):\n",
        "            if item in true_items:\n",
        "                dcg += 1 / np.log2(i + 2)  # Discounted gain\n",
        "        for i in range(min(len(true_items), k)):\n",
        "            idcg += 1 / np.log2(i + 2)  # Ideal discounted gain\n",
        "        ndcg = dcg / idcg if idcg > 0 else 0\n",
        "        ndcg_list.append(ndcg)\n",
        "\n",
        "    # Coverage: Proportion of items recommended at least once\n",
        "    coverage = np.sum(item_counts > 0) / n_movies\n",
        "\n",
        "    # Gini Index: Measure of inequality in item recommendation frequencies\n",
        "    sorted_counts = np.sort(item_counts)  # Sort item frequencies\n",
        "    n = len(sorted_counts)\n",
        "    gini = 1 - (2 / (n - 1)) * np.sum((n - np.arange(1, n + 1)) * sorted_counts) / np.sum(sorted_counts)\n",
        "\n",
        "    # Compute the average metrics across all users\n",
        "    metrics = {\n",
        "        \"Recall@K\": np.mean(recall_list),\n",
        "        \"Precision@K\": np.mean(precision_list),\n",
        "        \"NDCG@K\": np.mean(ndcg_list),\n",
        "        \"Gini Index\": gini,\n",
        "        \"Coverage\": coverage\n",
        "    }\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **1M Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vruNpn8BOmg",
        "outputId": "3aa9eaa3-0246-483e-f072-931a88510e0f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_70115/207279230.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  df_ratings = pd.read_csv(df_ratings_path, delimiter='::', header=None)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6040 3706\n"
          ]
        }
      ],
      "source": [
        "df_ratings_path = \"\" # insert here the path of 'ratings.csv' dataset\n",
        "\n",
        "# Load the ratings dataset\n",
        "df_ratings = pd.read_csv(df_ratings_path, delimiter='::', header=None)\n",
        "df_ratings.columns = ['userId', 'movieId', 'rating', 'timestamp']\n",
        "\n",
        "# Subtract 1 from userId and movieId to make them zero-based\n",
        "df_ratings['userId'] = df_ratings['userId'] - 1\n",
        "df_ratings['movieId'] = df_ratings['movieId'] - 1\n",
        "\n",
        "# -- Movie IDs are not sequential, so they need to be remapped\n",
        "# Get unique movie IDs and sort them\n",
        "unique_movieIds = sorted(df_ratings['movieId'].unique())\n",
        "# Create a dictionary mapping original movie IDs to new sequential IDs\n",
        "movieId_mapping = {old_id: new_id for new_id, old_id in enumerate(unique_movieIds)}\n",
        "# Apply the mapping to the 'movieId' column\n",
        "df_ratings['movieId'] = df_ratings['movieId'].map(movieId_mapping)\n",
        "\n",
        "n_users =  len(df_ratings['userId'].unique())  # Total number of users\n",
        "n_movies = len(df_ratings['movieId'].unique())  # Total number of unique movies\n",
        "print(n_users, n_movies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1/20, Train RMSE: 0.9458, Val RMSE: 0.9219\n",
            "Iteration 2/20, Train RMSE: 0.9105, Val RMSE: 0.9163\n",
            "Iteration 3/20, Train RMSE: 0.9016, Val RMSE: 0.9058\n",
            "Iteration 4/20, Train RMSE: 0.8802, Val RMSE: 0.8877\n",
            "Iteration 5/20, Train RMSE: 0.8498, Val RMSE: 0.8719\n",
            "Iteration 6/20, Train RMSE: 0.8119, Val RMSE: 0.8610\n",
            "Iteration 7/20, Train RMSE: 0.7674, Val RMSE: 0.8556\n",
            "Iteration 8/20, Train RMSE: 0.7178, Val RMSE: 0.8573\n",
            "Iteration 9/20, Train RMSE: 0.6688, Val RMSE: 0.8640\n",
            "Iteration 10/20, Train RMSE: 0.6242, Val RMSE: 0.8731\n",
            "Iteration 11/20, Train RMSE: 0.5853, Val RMSE: 0.8830\n",
            "Iteration 12/20, Train RMSE: 0.5528, Val RMSE: 0.8931\n",
            "Early stopping!\n",
            "{'Recall@K': 0.04112499771117983, 'Precision@K': 0.05771778734680358, 'NDCG@K': 0.06536283639007044, 'Gini Index': 0.9835295222028368, 'Coverage': 0.13437668645439826}\n"
          ]
        }
      ],
      "source": [
        "train_data, test_data = prepare_data(df_ratings)\n",
        "\n",
        "model = ImprovedMatrixFactorizationWithBias(\n",
        "    alpha=0.001,           # Learning rate for gradient descent\n",
        "    iterations=20,         # Number of training epochs\n",
        "    num_latent=100,        # Dimensionality of latent factors\n",
        "    lambda_reg=0.02,       # Regularization parameter to prevent overfitting\n",
        "    num_users=n_users,     # Total number of users in the dataset\n",
        "    num_items=n_movies     # Total number of movies in the dataset\n",
        ")\n",
        "model.fit(train_data, test_data)\n",
        "\n",
        "metrics = calculate_all_metrics(model, test_data, n_movies, k=10)\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **100K dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "943 1682\n"
          ]
        }
      ],
      "source": [
        "df_ratings_path = \"\" # insert here the path of 'ratings.csv' dataset\n",
        "\n",
        "# Load the ratings dataset\n",
        "df_ratings = pd.read_csv(df_ratings_path, delimiter='\\t', header=None)\n",
        "df_ratings.columns = ['userId', 'movieId', 'rating', 'timestamp']\n",
        "\n",
        "# Preprocessing\n",
        "df_ratings['userId'] = df_ratings['userId'] - 1\n",
        "df_ratings['movieId'] = df_ratings['movieId'] - 1\n",
        "\n",
        "n_users =  len(df_ratings['userId'].unique())  # Total number of users\n",
        "n_movies = len(df_ratings['movieId'].unique())  # Total number of unique movies\n",
        "print(n_users, n_movies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1/20, Train RMSE: 1.0071, Val RMSE: 0.9622\n",
            "Iteration 2/20, Train RMSE: 0.9465, Val RMSE: 0.9474\n",
            "Iteration 3/20, Train RMSE: 0.9302, Val RMSE: 0.9431\n",
            "Iteration 4/20, Train RMSE: 0.9183, Val RMSE: 0.9396\n",
            "Iteration 5/20, Train RMSE: 0.9015, Val RMSE: 0.9345\n",
            "Iteration 6/20, Train RMSE: 0.8747, Val RMSE: 0.9274\n",
            "Iteration 7/20, Train RMSE: 0.8356, Val RMSE: 0.9211\n",
            "Iteration 8/20, Train RMSE: 0.7877, Val RMSE: 0.9169\n",
            "Iteration 9/20, Train RMSE: 0.7332, Val RMSE: 0.9161\n",
            "Iteration 10/20, Train RMSE: 0.6752, Val RMSE: 0.9182\n",
            "Iteration 11/20, Train RMSE: 0.6180, Val RMSE: 0.9226\n",
            "Iteration 12/20, Train RMSE: 0.5625, Val RMSE: 0.9287\n",
            "Iteration 13/20, Train RMSE: 0.5122, Val RMSE: 0.9345\n",
            "Iteration 14/20, Train RMSE: 0.4663, Val RMSE: 0.9409\n",
            "Early stopping!\n",
            "{'Recall@K': 0.0375973705215825, 'Precision@K': 0.04436170212765958, 'NDCG@K': 0.0489784999091515, 'Gini Index': 0.9776633715999848, 'Coverage': 0.11831153388822829}\n"
          ]
        }
      ],
      "source": [
        "train_data, test_data = prepare_data(df_ratings)\n",
        "\n",
        "model = ImprovedMatrixFactorizationWithBias(\n",
        "    alpha=0.001,           # Learning rate for gradient descent\n",
        "    iterations=20,         # Number of training epochs\n",
        "    num_latent=100,        # Dimensionality of latent factors\n",
        "    lambda_reg=0.02,       # Regularization parameter to prevent overfitting\n",
        "    num_users=n_users,     # Total number of users in the dataset\n",
        "    num_items=n_movies     # Total number of movies in the dataset\n",
        ")\n",
        "model.fit(train_data, test_data)\n",
        "\n",
        "metrics = calculate_all_metrics(model, test_data, n_movies, k=10)\n",
        "print(metrics)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
